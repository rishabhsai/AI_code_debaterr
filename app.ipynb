{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishabhsai/AI_code_debaterr/blob/main/app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install Python libraries (including PDF generation)\n",
        "!pip install -q -U openai google-generativeai gradio reportlab markdown\n",
        "\n",
        "# 2. Import libraries and configure clients\n",
        "import openai\n",
        "import google.generativeai as genai\n",
        "import gradio as gr\n",
        "from google.colab import userdata\n",
        "import io\n",
        "import base64\n",
        "from datetime import datetime\n",
        "from reportlab.lib.pagesizes import letter, A4\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Preformatted\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.lib.units import inch\n",
        "from reportlab.lib.colors import HexColor\n",
        "import re\n",
        "\n",
        "# Initialize clients to None\n",
        "openai_client = None\n",
        "gemini_model = None\n",
        "\n",
        "# Configure OpenAI client\n",
        "try:\n",
        "    api_key = userdata.get('OPENAI_API_KEY')\n",
        "    openai_client = openai.OpenAI(api_key=api_key)\n",
        "    print(\"âœ… OpenAI (GPT-4o mini) client configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ OpenAI setup failed: {e}\")\n",
        "    print(\"Make sure you have added 'OPENAI_API_KEY' to Colab Secrets (ğŸ”‘)\")\n",
        "\n",
        "# Configure Gemini client\n",
        "try:\n",
        "    api_key = userdata.get('GEMINI_API_KEY')\n",
        "    genai.configure(api_key=api_key)\n",
        "    gemini_model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "    print(\"âœ… Gemini client configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Gemini setup failed: {e}\")\n",
        "    print(\"Make sure you have added 'GEMINI_API_KEY' to Colab Secrets (ğŸ”‘)\")\n",
        "\n",
        "print(\"\\nğŸš€ Setup complete! Run the next cell to launch the app.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOS_H6j5y4Hg",
        "outputId": "00842802-19d2-4faa-86be-5b6a4ba05eb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m734.3/734.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.3/54.3 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.6/323.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… OpenAI (GPT-4o mini) client configured successfully.\n",
            "âœ… Gemini client configured successfully.\n",
            "\n",
            "ğŸš€ Setup complete! Run the next cell to launch the app.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pdf_report(debate_log, final_code, user_prompt):\n",
        "\n",
        "    # Creates a PDF report of the entire debate conversation including all reviewer notes\n",
        "\n",
        "    buffer = io.BytesIO()\n",
        "\n",
        "    # Create the PDF document\n",
        "    doc = SimpleDocTemplate(buffer, pagesize=A4,\n",
        "                          rightMargin=72, leftMargin=72,\n",
        "                          topMargin=72, bottomMargin=18)\n",
        "\n",
        "    # Get styles\n",
        "    styles = getSampleStyleSheet()\n",
        "\n",
        "    title_style = ParagraphStyle(\n",
        "        'CustomTitle',\n",
        "        parent=styles['Heading1'],\n",
        "        fontSize=24,\n",
        "        spaceAfter=30,\n",
        "        textColor=HexColor('#2E86AB'),\n",
        "        alignment=1  # Center alignment\n",
        "    )\n",
        "\n",
        "    heading_style = ParagraphStyle(\n",
        "        'CustomHeading',\n",
        "        parent=styles['Heading2'],\n",
        "        fontSize=16,\n",
        "        spaceAfter=12,\n",
        "        textColor=HexColor('#A23B72'),\n",
        "        spaceBefore=20\n",
        "    )\n",
        "\n",
        "    subheading_style = ParagraphStyle(\n",
        "        'CustomSubHeading',\n",
        "        parent=styles['Heading3'],\n",
        "        fontSize=14,\n",
        "        spaceAfter=8,\n",
        "        spaceBefore=12,\n",
        "        textColor=HexColor('#1F4E79')\n",
        "    )\n",
        "\n",
        "    code_style = ParagraphStyle(\n",
        "        'CodeStyle',\n",
        "        parent=styles['Code'],\n",
        "        fontSize=9,\n",
        "        leftIndent=20,\n",
        "        backgroundColor=HexColor('#F8F8F8'),\n",
        "        borderColor=HexColor('#DDDDDD'),\n",
        "        borderWidth=1,\n",
        "        borderPadding=8,\n",
        "        fontName='Courier'\n",
        "    )\n",
        "\n",
        "    review_style = ParagraphStyle(\n",
        "        'ReviewStyle',\n",
        "        parent=styles['Normal'],\n",
        "        fontSize=11,\n",
        "        leftIndent=15,\n",
        "        rightIndent=15,\n",
        "        backgroundColor=HexColor('#FFF9E6'),\n",
        "        borderColor=HexColor('#FFD700'),\n",
        "        borderWidth=1,\n",
        "        borderPadding=10,\n",
        "        spaceBefore=5,\n",
        "        spaceAfter=10\n",
        "    )\n",
        "\n",
        "    # list to hold all elements\n",
        "    story = []\n",
        "\n",
        "    # Title\n",
        "    story.append(Paragraph(\"ğŸ¤–ğŸ’ AI Code Generation Debate Report\", title_style))\n",
        "    story.append(Spacer(1, 20))\n",
        "\n",
        "    # Metadata\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    story.append(Paragraph(f\"<b>Generated:</b> {timestamp}\", styles['Normal']))\n",
        "    story.append(Paragraph(f\"<b>Models:</b> GPT-4o mini (Coder) vs Gemini 1.5 Flash (Reviewer)\", styles['Normal']))\n",
        "    story.append(Spacer(1, 20))\n",
        "\n",
        "    # User Request\n",
        "    story.append(Paragraph(\"ğŸ“ User Request\", heading_style))\n",
        "    story.append(Paragraph(user_prompt, styles['Normal']))\n",
        "    story.append(Spacer(1, 20))\n",
        "\n",
        "    # Parse the debate log more carefully\n",
        "    lines = debate_log.split('\\n')\n",
        "    current_section = None\n",
        "    current_content = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(lines):\n",
        "        line = lines[i].strip()\n",
        "\n",
        "        # Check for round headers\n",
        "        if line.startswith('--- **Round') and line.endswith('** ---'):\n",
        "            # Process previous section if exists\n",
        "            if current_section and current_content:\n",
        "                process_section(story, current_section, current_content, code_style, review_style, subheading_style, styles)\n",
        "\n",
        "            # Start new round\n",
        "            round_match = re.search(r'Round (\\d+)', line)\n",
        "            if round_match:\n",
        "                round_num = round_match.group(1)\n",
        "                story.append(Paragraph(f\"ğŸ”„ Round {round_num}\", heading_style))\n",
        "                story.append(Spacer(1, 10))\n",
        "            current_section = None\n",
        "            current_content = []\n",
        "\n",
        "        # Check for coder section\n",
        "        elif line.startswith('**ğŸ¤– Coder Agent (GPT-4o mini):**'):\n",
        "            # Process previous section if exists\n",
        "            if current_section and current_content:\n",
        "                process_section(story, current_section, current_content, code_style, review_style, subheading_style, styles)\n",
        "\n",
        "            current_section = 'coder'\n",
        "            current_content = []\n",
        "\n",
        "        # Check for reviewer section\n",
        "        elif line.startswith('**ğŸ’ Reviewer Agent (Gemini):**'):\n",
        "            # Process previous section if exists\n",
        "            if current_section and current_content:\n",
        "                process_section(story, current_section, current_content, code_style, review_style, subheading_style, styles)\n",
        "\n",
        "            current_section = 'reviewer'\n",
        "            current_content = []\n",
        "\n",
        "        # Check for agreement\n",
        "        elif 'Agreement reached' in line:\n",
        "            # Process previous section if exists\n",
        "            if current_section and current_content:\n",
        "                process_section(story, current_section, current_content, code_style, review_style, subheading_style, styles)\n",
        "\n",
        "            story.append(Paragraph(\"ğŸ‰ Agreement Reached!\", subheading_style))\n",
        "            story.append(Paragraph(\"The reviewer is satisfied with the code.\", styles['Normal']))\n",
        "            story.append(Spacer(1, 15))\n",
        "            current_section = None\n",
        "            current_content = []\n",
        "\n",
        "        # Collect content for current section\n",
        "        elif current_section and line:\n",
        "            current_content.append(line)\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    # Process final section if exists\n",
        "    if current_section and current_content:\n",
        "        process_section(story, current_section, current_content, code_style, review_style, subheading_style, styles)\n",
        "\n",
        "    # Final Code Section\n",
        "    story.append(Paragraph(\"âœ… Final Agreed-Upon Code\", heading_style))\n",
        "    story.append(Preformatted(final_code, code_style))\n",
        "\n",
        "    # Build PDF\n",
        "    doc.build(story)\n",
        "\n",
        "    # Get the PDF data\n",
        "    buffer.seek(0)\n",
        "    pdf_data = buffer.getvalue()\n",
        "    buffer.close()\n",
        "\n",
        "    return pdf_data\n",
        "\n",
        "def process_section(story, section_type, content, code_style, review_style, subheading_style, styles):\n",
        "    \"\"\"\n",
        "    Helper function to process different sections of the debate\n",
        "    \"\"\"\n",
        "    if section_type == 'coder':\n",
        "        story.append(Paragraph(\"ğŸ¤– Coder Agent (GPT-4o mini)\", subheading_style))\n",
        "\n",
        "        # Look for code blocks\n",
        "        content_text = '\\n'.join(content)\n",
        "        if '```python' in content_text:\n",
        "            # Extract code from markdown\n",
        "            code_match = re.search(r'```python\\n(.*?)\\n```', content_text, re.DOTALL)\n",
        "            if code_match:\n",
        "                code = code_match.group(1)\n",
        "                story.append(Preformatted(code, code_style))\n",
        "        else:\n",
        "            # If no code blocks, treat as regular text\n",
        "            clean_content = '\\n'.join(content).replace('```python', '').replace('```', '')\n",
        "            if clean_content.strip():\n",
        "                story.append(Preformatted(clean_content.strip(), code_style))\n",
        "\n",
        "        story.append(Spacer(1, 15))\n",
        "\n",
        "    elif section_type == 'reviewer':\n",
        "        story.append(Paragraph(\"ğŸ’ Reviewer Agent (Gemini) - Feedback\", subheading_style))\n",
        "\n",
        "        # Clean up reviewer content\n",
        "        review_text = '\\n'.join(content)\n",
        "        # Remove any markdown formatting\n",
        "        review_text = review_text.replace('**', '').replace('*', '').strip()\n",
        "\n",
        "        if review_text:\n",
        "            # Split into paragraphs for better formatting\n",
        "            paragraphs = review_text.split('\\n\\n')\n",
        "            for para in paragraphs:\n",
        "                if para.strip():\n",
        "                    story.append(Paragraph(para.strip(), review_style))\n",
        "\n",
        "        story.append(Spacer(1, 15))\n",
        "\n",
        "def generate_code_with_debate(user_prompt, rounds=3):\n",
        "    # Check if clients are configured\n",
        "    if not openai_client or not gemini_model:\n",
        "        error_msg = \"âŒ Client setup failed. Please check the setup cell output and try again.\"\n",
        "        return \"Error: Clients not configured.\", error_msg, None\n",
        "\n",
        "    debate_log = f\"**User Request:**\\n{user_prompt}\\n\\n\"\n",
        "    print(f\"Starting debate for: {user_prompt}\")\n",
        "\n",
        "    coder_system_prompt = \"\"\"\n",
        "    You are an expert Python programmer (Coder Agent). Your sole purpose is to\n",
        "    write clean, efficient, and correct Python code based on the user's request\n",
        "    and the reviewer's feedback. Provide only the raw code without any explanations\n",
        "    or markdown formatting. Just return the pure Python code.\n",
        "    \"\"\"\n",
        "\n",
        "    reviewer_system_prompt = \"\"\"\n",
        "    You are a meticulous and critical code reviewer (Reviewer Agent). Your job\n",
        "    is to find flaws in the provided Python code. Check for bugs, edge cases,\n",
        "    non-standard practices, and areas for improvement in performance or\n",
        "    readability. Provide your feedback as a clear, concise list. If the code\n",
        "    is perfect, respond with \"The code looks good. No changes needed.\"\n",
        "    \"\"\"\n",
        "\n",
        "    current_code = \"No code generated yet.\"\n",
        "\n",
        "    # OpenAI conversation history\n",
        "    coder_messages = [\n",
        "        {\"role\": \"system\", \"content\": coder_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "\n",
        "    for i in range(rounds):\n",
        "        round_num = i + 1\n",
        "        debate_log += f\"--- **Round {round_num}** ---\\n\\n\"\n",
        "        print(f\"--- Round {round_num} ---\")\n",
        "\n",
        "        # Coder's turn (GPT-4o mini)\n",
        "        try:\n",
        "            print(\"ğŸ¤– Coder (GPT-4o mini) is thinking...\")\n",
        "            coder_response = openai_client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=coder_messages,\n",
        "                temperature=0.7\n",
        "            )\n",
        "            current_code = coder_response.choices[0].message.content.strip()\n",
        "\n",
        "            # Clean up code block formatting if present\n",
        "            if \"```python\" in current_code:\n",
        "                current_code = current_code.split(\"```python\\n\")[1].split(\"```\")[0]\n",
        "            elif \"```\" in current_code:\n",
        "                current_code = current_code.split(\"```\")[1].split(\"```\")[0]\n",
        "\n",
        "            debate_log += f\"**ğŸ¤– Coder Agent (GPT-4o mini):**\\n```python\\n{current_code}\\n```\\n\\n\"\n",
        "\n",
        "            # Add to conversation history\n",
        "            coder_messages.append({\"role\": \"assistant\", \"content\": current_code})\n",
        "\n",
        "        except Exception as e:\n",
        "            debate_log += f\"**âŒ Error calling Coder Agent:** {e}\\n\"\n",
        "            print(f\"Error with Coder: {e}\")\n",
        "            break\n",
        "\n",
        "        # Reviewer's turn (Gemini)\n",
        "        try:\n",
        "            print(\"ğŸ’ Reviewer (Gemini) is thinking...\")\n",
        "\n",
        "            # Create the review prompt for Gemini\n",
        "            review_prompt = f\"{reviewer_system_prompt}\\n\\nPlease review this Python code:\\n\\n{current_code}\"\n",
        "\n",
        "            reviewer_response = gemini_model.generate_content(review_prompt)\n",
        "            review = reviewer_response.text.strip()\n",
        "\n",
        "            debate_log += f\"**ğŸ’ Reviewer Agent (Gemini):**\\n{review}\\n\\n\"\n",
        "\n",
        "        except Exception as e:\n",
        "            debate_log += f\"**âŒ Error calling Reviewer Agent:** {e}\\n\"\n",
        "            print(f\"Error with Reviewer: {e}\")\n",
        "            break\n",
        "\n",
        "        # Check for agreement\n",
        "        if \"no changes needed\" in review.lower() or \"looks good\" in review.lower():\n",
        "            debate_log += \"**ğŸ‰ Agreement reached!** The reviewer is satisfied with the code.\"\n",
        "            print(\"ğŸ‰ Agreement reached!\")\n",
        "            break\n",
        "\n",
        "        # Prepare for Coder's next turn\n",
        "        coder_messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Please revise the code based on this review feedback:\\n\\n{review}\"\n",
        "        })\n",
        "\n",
        "    print(\"âœ… Debate finished.\")\n",
        "\n",
        "    # Generate PDF\n",
        "    try:\n",
        "        pdf_data = create_pdf_report(debate_log, current_code, user_prompt)\n",
        "        # Create a temporary file for download\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"ai_code_debate_{timestamp}.pdf\"\n",
        "\n",
        "        # Save to a temporary location that Gradio can access\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(pdf_data)\n",
        "\n",
        "        return current_code, debate_log, filename\n",
        "    except Exception as e:\n",
        "        print(f\"PDF generation error: {e}\")\n",
        "        return current_code, debate_log, None\n",
        "\n",
        "# Create the Gradio Interface\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"AI Code Debate\") as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # ğŸ¤–ğŸ’ AI Code Generation Debate\n",
        "\n",
        "        **GPT-4o mini** (Coder) vs **Gemini 1.5 Flash** (Reviewer)\n",
        "\n",
        "        Watch two different AI models collaborate! GPT-4o mini will write the initial code,\n",
        "        and Gemini will review and suggest improvements through multiple rounds of discussion.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        user_input = gr.Textbox(\n",
        "            label=\"ğŸ’­ Your Code Request\",\n",
        "            placeholder=\"e.g., Write a Python function to find all prime numbers up to n using the Sieve of Eratosthenes\",\n",
        "            lines=3,\n",
        "        )\n",
        "\n",
        "    with gr.Row():\n",
        "        rounds_slider = gr.Slider(\n",
        "            minimum=1,\n",
        "            maximum=5,\n",
        "            value=3,\n",
        "            step=1,\n",
        "            label=\"ğŸ”„ Number of Debate Rounds\"\n",
        "        )\n",
        "        submit_button = gr.Button(\"ğŸš€ Generate Code\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### ğŸ“œ Debate Log\")\n",
        "            log_output = gr.Markdown()\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### âœ… Final Agreed-Upon Code\")\n",
        "            code_output = gr.Code(language=\"python\")\n",
        "\n",
        "    # PDF Download Section\n",
        "    with gr.Row():\n",
        "        pdf_download = gr.File(label=\"ğŸ“„ Download Debate Report (PDF)\", visible=False)\n",
        "\n",
        "    def handle_debate_and_pdf(user_prompt, rounds):\n",
        "        code, log, pdf_file = generate_code_with_debate(user_prompt, rounds)\n",
        "        if pdf_file:\n",
        "            return code, log, gr.File(value=pdf_file, visible=True)\n",
        "        else:\n",
        "            return code, log, gr.File(visible=False)\n",
        "\n",
        "    submit_button.click(\n",
        "        fn=handle_debate_and_pdf,\n",
        "        inputs=[user_input, rounds_slider],\n",
        "        outputs=[code_output, log_output, pdf_download],\n",
        "    )\n",
        "\n",
        "    # Add some example prompts\n",
        "    gr.Examples(\n",
        "        examples=[\n",
        "            [\"Write a Python function to calculate fibonacci numbers recursively\"],\n",
        "            [\"Create a class for a simple calculator with basic operations\"],\n",
        "            [\"Write a function to validate email addresses using regex\"],\n",
        "            [\"Implement a binary search algorithm\"],\n",
        "            [\"Create a function to merge two sorted lists\"],\n",
        "            [\"Write a decorator to measure function execution time\"],\n",
        "        ],\n",
        "        inputs=user_input,\n",
        "    )\n",
        "\n",
        "# Launch the app\n",
        "print(\"ğŸš€ Launching Gradio app...\")\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "GTDs7SOBzIgu",
        "outputId": "37c179d5-872b-480d-88f1-5068b8c5f51f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Launching Gradio app...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://b357bc7f479c6313dd.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://b357bc7f479c6313dd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting debate for: Implement a binary search algorithm\n",
            "--- Round 1 ---\n",
            "ğŸ¤– Coder (GPT-4o mini) is thinking...\n",
            "ğŸ’ Reviewer (Gemini) is thinking...\n",
            "--- Round 2 ---\n",
            "ğŸ¤– Coder (GPT-4o mini) is thinking...\n",
            "ğŸ’ Reviewer (Gemini) is thinking...\n",
            "--- Round 3 ---\n",
            "ğŸ¤– Coder (GPT-4o mini) is thinking...\n",
            "ğŸ’ Reviewer (Gemini) is thinking...\n",
            "âœ… Debate finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UDeCsNQw7INh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}